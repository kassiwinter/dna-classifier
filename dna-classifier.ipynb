{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9be79b-7a24-4de3-92f2-8c8b9fedbce2",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "272f32a7-724b-4b8e-8a6e-4023c2d7bc7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72caa45-4b1d-45fb-91a7-bc35faffec6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from joblib import parallel_backend\n",
    "from Bio import SeqIO\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from itertools import permutations\n",
    "import plotly.graph_objs as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display= 'diagram')\n",
    "from sklearn.metrics import classification_report, plot_roc_curve, plot_confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.svm import SVC\n",
    "from src.KMerTransformers import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb019261-42f9-46f5-8d4e-8fc5a619f6ed",
   "metadata": {},
   "source": [
    "## Import DNA Sequence Data\n",
    "\n",
    "I will be using the BioPython library to parse through fasta files found on the RefSeq database.\n",
    "\n",
    "Each file contains sequences from the genome of a species. They are each tens of thousands of rows long, but I will limit each dataframe to 20000 rows per species.\n",
    "\n",
    "Let's look at one record as an example of the data that is being parsed in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d0a92-f162-41c2-a007-c7a95b6e72b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_record in SeqIO.parse(\"data/GCF_000001405.40_GRCh38.p14_cds_from_genomic.fna\", \"fasta\"): #human fasta file\n",
    "    print(seq_record)\n",
    "    break #break after printing one record"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92560107-e06d-4f46-864f-a19e9de9832d",
   "metadata": {},
   "source": [
    "This project will be using using the sequence (Seq) as data in the model, but let's define a helper function that will collect a few more columns of data, just in case I want to reference them later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c228a9-c892-4473-90fa-a050e2043260",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_from_genome_file(filename, target_name, n=None):\n",
    "    \n",
    "    i=0\n",
    "    seqs = []\n",
    "    genes = []\n",
    "    proteins = []\n",
    "    target = []\n",
    "    \n",
    "    for seq_record in SeqIO.parse(filename, \"fasta\"):\n",
    "        \n",
    "        seqs.append(''.join(seq_record.seq)) #append the sequence data\n",
    "        genes.append(seq_record.description.split('[')[1][5:-2]) #append gene name\n",
    "        proteins.append(seq_record.description.split('[')[3][8:-2]) #append protein description\n",
    "        target.append(target_name) #append the class name ('human' or otherwise)\n",
    "        \n",
    "        if n != None:\n",
    "            if i < n:\n",
    "                i+=1\n",
    "            if i >= n:\n",
    "                break\n",
    "            \n",
    "    df = pd.concat([pd.DataFrame(seqs),pd.DataFrame(genes), pd.DataFrame(proteins), pd.DataFrame(target)], axis=1)\n",
    "    df.columns = ['seq','gene','protein', 'target']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ff5c01-0439-4cd1-9b6c-0fc1de4d0367",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dataframes for each genome file\n",
    "human_df = parse_from_genome_file(\"data/GCF_000001405.40_GRCh38.p14_cds_from_genomic.fna\", 'human', n=20000)\n",
    "chimp_df = parse_from_genome_file('data/GCF_002880755.1_Clint_PTRv2_cds_from_genomic.fna', 'chimp', n=20000)\n",
    "dolphin_df = parse_from_genome_file('data/GCF_011762595.1_mTurTru1.mat.Y_cds_from_genomic.fna', 'dolphin', n=20000)\n",
    "oak_df = parse_from_genome_file('data/GCF_001633185.2_ValleyOak3.2_cds_from_genomic.fna', 'oak', n=20000)\n",
    "mushroom_df = parse_from_genome_file('data/GCF_017499595.1_MGC_Penvy_1_cds_from_genomic.fna', 'mushroom', n=20000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d732559-fb8a-4f21-96cc-b1016defff31",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f7937f-9585-47e7-88b6-07e6bdde4981",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df.loc[0,'seq']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56268153-1df0-45da-8bec-7ced666d00fc",
   "metadata": {},
   "source": [
    "## EDA\n",
    "\n",
    "Before I do any analysis, I will add all of the species from the DNA sequence data I collected into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fff0071-106a-4de1-a52b-afe031ea2590",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df = pd.concat([human_df, chimp_df, dolphin_df, oak_df, mushroom_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3607f55f-c5b9-473c-9bc7-efe6869527e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ebc48b-9717-43eb-b574-51915c43f5ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b4cf9a7-645e-486b-b0c1-980c38077f1b",
   "metadata": {},
   "source": [
    "The first thing we will want to see are the metrics of the DNA sequence itself. Let's look at the length of the sequence, grouped by species."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a874fe9f-9e35-4c99-b448-694d60acbd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df['seq_length'] = species_compare_df.seq.apply(lambda x: len(x))\n",
    "print('DNA Sequence Length Info by Species')\n",
    "species_compare_df.groupby('target').seq_length.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70213456-019a-4c79-8029-d0cf5ed426e2",
   "metadata": {},
   "source": [
    "We can now determine that the mean length of each sequence between targets is relatively the same, apart from the Oak Tree and Mushroom genomic data.\n",
    "\n",
    "Using the custom KMerTransformer function that I imported, I can transform the data and do some basic visual analysis. I will start with k=3 so I don't have too many columns and I can see the data together easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a7c925-8aba-4011-b5e7-eac18d478e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_transformer = KMerTransformer(k=3, verbose=False)\n",
    "kmer_matrix = kmer_transformer.fit_transform(species_compare_df)\n",
    "kmer_matrix = pd.concat([kmer_matrix, species_compare_df.target], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15b1e19-78c8-4a76-93c0-bfe09b513118",
   "metadata": {},
   "source": [
    "Let's quickly inspect that the data is populating in the columns correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c5c0bc-285b-4317-9de6-8678b8c250aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_matrix.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e4279f-854a-4e3d-b052-3076bd6b32da",
   "metadata": {},
   "source": [
    "5 rows × 65 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b37b1-3704-4afd-a018-7cbda95c43c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot a heatmap of correlation between features\n",
    "kmer_corr = kmer_matrix.corr()\n",
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.title('Correlation between trimers', fontdict={'fontsize': 20})\n",
    "sns.set(font_scale=.6)\n",
    "sns.heatmap(kmer_corr, cmap=\"vlag\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cf0b3d-8c57-409d-a996-a9a064219171",
   "metadata": {},
   "source": [
    "This heatmap shows that there is high correlation between most of our features. This will affect our model, and I may be able to find a way to extract features that have no correlation in common, or classifiers that can deal with a high levele of colinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8b4722-6acb-4545-a95f-c5e404481f29",
   "metadata": {},
   "source": [
    "**Mean Values for each Trimer Grouped by Species**\n",
    "\n",
    "Now, let's take a look at the mean values for every column group by the 4 species' DNA I added to this dataframe as an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23e20d-da2f-4b82-aae1-60d92eb34efe",
   "metadata": {},
   "source": [
    "5 rows × 64 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975bb241-e6aa-4e4d-ac8f-02b640740159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dimension of each subplot and the final graph\n",
    "rows = 16\n",
    "cols = 4\n",
    "fig_cols = 5\n",
    "fig_rows = 1\n",
    "\n",
    "#get the total max and min for scaling later\n",
    "mean_agg = mean_agg_df\n",
    "mean_max = mean_agg.max().max()\n",
    "mean_min = mean_agg.min().min()\n",
    "\n",
    "#Extract the title for each subplot, and the trimers in each matrix\n",
    "mean_agg_index = [f'{x.title()} Genome' for x in mean_agg.index]\n",
    "kmer_index = np.array([x[0] for x in mean_agg.columns]).reshape(rows, cols)\n",
    "#scale the data with MinMax\n",
    "mean_agg = (np.array(mean_agg) - mean_min) / (mean_max - mean_min)\n",
    "\n",
    "#plot the graph\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=mean_agg_index)\n",
    "layout = {'title':'Scaled Mean Values of Each Trimer Grouped by Genome', 'width':1200, 'height':600}\n",
    "for row_index in range(fig_rows):\n",
    "    for col_index in range(fig_cols):\n",
    "        index = col_index + fig_rows*row_index\n",
    "        array = np.array(mean_agg[index]).astype(float).reshape(rows, cols)\n",
    "        #draw a heatmap for genome\n",
    "        fig.add_trace(go.Heatmap(z=array, coloraxis='coloraxis'), row=row_index+1, col=col_index+1)\n",
    "        #draw each trimer over each square in each subplot\n",
    "        for k in range(rows):\n",
    "            for j in range(cols):\n",
    "                fig.add_annotation(text=np.array(kmer_index).reshape(rows, cols)[k][j], \n",
    "                                   x=j, y=k, showarrow=False, row=row_index+1, col=col_index+1, font=dict(size=9, color=\"#222299\"))\n",
    "        layout[f'xaxis{index+1 if index > 0 else \"\"}'] = dict(visible=False)\n",
    "        layout[f'yaxis{index+1 if index > 0 else \"\"}'] = dict(autorange='reversed',visible=False)\n",
    "\n",
    "fig.update_layout(layout)\n",
    "fig.update_coloraxes(colorscale = 'bupu')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5bbae2-1657-4b04-b336-b2d3d6a2d1d3",
   "metadata": {},
   "source": [
    "In this graph, we are visualizing those mean values for every columns arranged as a 2-dimensional grid. Each subplot is one of the four species, and now we can visually compare the means of their values together. It looks like there are some patterns emerging already. For the mammalian genomes (human, chimp, and dolphin) there are a lot of areas that are visually similiar. Even some parts of the Oak genome are related to the three others, but for the most part it is easy to distinguish already that it is different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37e9614-49cc-4f99-b609-3f6fa2bf7dbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_similiarity_map = pd.DataFrame(cosine_similarity(np.array(mean_agg_df),np.array(mean_agg_df)), index=mean_agg_df.index, columns=mean_agg_df.index)\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "sns.set(font_scale=1)\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.heatmap(cosine_similiarity_map)\n",
    "plt.suptitle('Cosine Similarity between Genomes (Mean Values)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2702b8e-3c1d-4296-8468-ec234af2c7b6",
   "metadata": {},
   "source": [
    "We have visually seen the similiarity between our features in different classes, but I also compute the cosine similiarity between each class. Note that the bottom of this scale is still a value of high similiarity. It seems that all of our classes have a high value of cosine similiarity.\n",
    "\n",
    "Now let's take a look at a visualization of the variance in each genome's features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b991e477-0558-4729-a37e-3ee9f2c2d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "var_agg_df = kmer_matrix.groupby('target', sort=False).agg(['std'])\n",
    "var_agg_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a308b4c7-ef97-4729-a75f-4722ec57ce4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the dimension of each subplot and the final graph\n",
    "rows = 16\n",
    "cols = 4\n",
    "fig_cols = 5\n",
    "fig_rows = 1\n",
    "\n",
    "#Extract the title for each subplot, and the trimers in each matrix\n",
    "var_agg = var_agg_df\n",
    "var_agg_index = [f'{x.title()} Genome' for x in var_agg.index]\n",
    "kmer_index = np.array([x[0] for x in var_agg.columns]).reshape(rows, cols)\n",
    "var_agg = np.array(var_agg)\n",
    "           \n",
    "#plot the graph\n",
    "fig = make_subplots(rows=fig_rows, cols=fig_cols, subplot_titles=var_agg_index)\n",
    "layout = {'title':'Scaled Variance of Each Trimer Column Grouped by Genome', 'width':1200, 'height':600}\n",
    "for row_index in range(fig_rows):\n",
    "    for col_index in range(fig_cols):\n",
    "        index = col_index + fig_rows*row_index\n",
    "        array = np.array(var_agg[index]).astype(float).reshape(rows, cols)\n",
    "        #draw a heatmap for genome\n",
    "        fig.add_trace(go.Heatmap(z=array, coloraxis='coloraxis'), row=row_index+1, col=col_index+1)\n",
    "        #draw each trimer over each square in each subplot\n",
    "        for k in range(rows):\n",
    "            for j in range(cols):\n",
    "                fig.add_annotation(text=np.array(kmer_index).reshape(rows, cols)[k][j], \n",
    "                                   x=j, y=k, showarrow=False, row=row_index+1, col=col_index+1, font=dict(size=9, color=\"#222299\"))\n",
    "        layout[f'xaxis{index+1 if index > 0 else \"\"}'] = dict(visible=False)\n",
    "        layout[f'yaxis{index+1 if index > 0 else \"\"}'] = dict(autorange='reversed',visible=False)\n",
    "\n",
    "fig.update_layout(layout)\n",
    "fig.update_coloraxes(colorscale = 'tempo')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5cb381-36d6-49e5-9730-73166d5c8a38",
   "metadata": {},
   "source": [
    "Based on this visualization, we can see that some genomes have higher variance than others. It will be interesting to see if this has an impact on multiclass models, or could be explained during binary classification of two classes that have different levels of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9931673-f6ff-4b5c-9767-85e8c6dd7611",
   "metadata": {},
   "source": [
    "## Train Test Split\n",
    "\n",
    "I will want to do a multi-class model later on, but I will start with a 2 class dataframe to create a baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d93f018-85cd-4b60-bb4d-7b1d0e97d381",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_df = pd.concat([human_df, oak_df], axis=0).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b937a22-a137-466a-98a1-f2a9a12cd2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeler = LabelEncoder()\n",
    "\n",
    "X = baseline_df.drop(columns='target')\n",
    "y = labeler.fit_transform(baseline_df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e654af7d-92a6-45bc-a6df-b40d6fcb5ee8",
   "metadata": {},
   "source": [
    "Now that I have split the data into training and testing sets, I can transform the X's into the dataframe's we will use in the model using the custom KMerTransformer that was imported. I will start the baseline model with k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84733f3d-45dc-452a-8653-be78424d7722",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_transformer = KMerTransformer(k=3, verbose=False)\n",
    "X_train_trans = kmer_transformer.fit_transform(X_train)\n",
    "X_test_trans = kmer_transformer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb3ce2a-739f-49ac-9d18-ae76a676551b",
   "metadata": {},
   "source": [
    "## Baseline Model for Binary Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3e389c-ef04-44dd-8aab-5d9b1baf0a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "], verbose=True)\n",
    "baseline_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851e58e9-4e9a-4a90-813d-a0c9500f5b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_model.fit(X_train_trans, y_train)\n",
    "y_preds = baseline_model.predict(X_test_trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bd2c9f-bcea-452d-b1c8-5b6735f9e063",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))\n",
    "\n",
    "plot_confusion_matrix(baseline_model, X_test_trans, y_test)\n",
    "plt.show()\n",
    "\n",
    "plot_roc_curve(baseline_model, X_test_trans, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b9a1a5-6779-4cdf-9369-02f66355b4c0",
   "metadata": {},
   "source": [
    "## Human vs Chimpanzee DNA Binary Classifier\n",
    "\n",
    "We can assume that Human and Chimpanzee DNA is much more similar than Human and Oak DNA from our intial EDA and cosine similiarity tests. I will try to run the same LogisiticRegression classifier on just Human and Chimapanzee DNA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0095a684-e250-41d2-9beb-a7609929337d",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chimp_df = pd.concat([human_df, chimp_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "labeler = LabelEncoder()\n",
    "\n",
    "X = human_chimp_df.drop(columns='target')\n",
    "y = labeler.fit_transform(human_chimp_df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "kmer_transformer = KMerTransformer(k=3, verbose=False)\n",
    "X_train_trans = kmer_transformer.fit_transform(X_train)\n",
    "X_test_trans = kmer_transformer.transform(X_test)\n",
    "\n",
    "human_chimp_model = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "], verbose=True)\n",
    "\n",
    "human_chimp_model.fit(X_train_trans, y_train)\n",
    "y_preds = human_chimp_model.predict(X_test_trans)\n",
    "\n",
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad50548-d282-48ba-9560-d71199626efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(human_chimp_model, X_test_trans, y_test)\n",
    "plt.suptitle('Human DNA vs Chimpanzee DNA Logisitic Regression Confusion Matrix')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88710ebf-fc37-4ba4-89fe-7ca8660114c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(human_chimp_model, X_test_trans, y_test)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26ee279-0967-400d-8839-1b676c4218cf",
   "metadata": {},
   "source": [
    "The untuned Logisitic Regression is not accurate at all. A 53% accuracy score is barely more than just random chance because the two classes in the test set are balanced. This may be due to the high colinearity of our features. Since I am sticking with this feature set for now, I will try out different classifiers that may work better with high colinearity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224d356-e41b-4827-b04b-ade2a1efbeb2",
   "metadata": {},
   "source": [
    "## Grid Search for Best Binary Classifier, k=3\n",
    "\n",
    "Since Logisitic Regression does not seem to be a good classifier for our data, I will implement a grid search on different parameters for many different classifiers to see if we can find one that will fit the data better. The model is still using the same training and testing data trimers (k=3) from the KMerTransformer. I will assume that with a higher k, we will see a better accuracy score, but it will also take longer to fit each model. If this grid search finds good classifiers, I could increase the k in another grid search, but on a smaller set of parameters. First let's just try the untuned classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b05d6-ffc4-4b3a-ad38-108f875d8d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = len(X_train_trans.columns)\n",
    "\n",
    "human_chimp_model = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "params = [\n",
    "    {'classifier':[LogisticRegression(random_state=RANDOM_STATE)]},\n",
    "    {'classifier':[GaussianNB()]},\n",
    "    {'classifier':[KNeighborsClassifier()]},\n",
    "    {'classifier':[RandomForestClassifier(random_state=RANDOM_STATE)]},\n",
    "    {'classifier':[ExtraTreesClassifier(random_state=RANDOM_STATE)]},\n",
    "    {'classifier':[XGBClassifier(random_state=RANDOM_STATE)]},\n",
    "    {'classifier':[GradientBoostingClassifier(random_state=RANDOM_STATE)]},\n",
    "    {'classifier':[SVC(random_state=RANDOM_STATE)]}\n",
    "]\n",
    "\n",
    "grid_searcher_clf = GridSearchCV(human_chimp_model, params, cv=3, scoring='accuracy', verbose=1)\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    grid_searcher_clf.fit(X_train_trans, y_train)\n",
    "    best_est = grid_searcher_clf.best_estimator_\n",
    "    y_preds = best_est.predict(X_test_trans)\n",
    "    \n",
    "print('Best Estimator:')\n",
    "print(best_est)\n",
    "\n",
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519d45ef-0be4-4714-8679-58c6641de27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "sns.set(font_scale = .65)\n",
    "cv_results = list(grid_searcher_clf.cv_results_['mean_test_score'])\n",
    "cv_params = [re.sub(r'\\(.*?\\)', \"\", str(x['classifier']).replace(\"\\n\", \"\")) for x in grid_searcher_clf.cv_results_['params']]\n",
    "sns.barplot(cv_params, cv_results)\n",
    "fig.suptitle('Accuracy of Different Classifiers in GridSearchCV ')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "553310b1-9f82-495d-b309-cdb970b5c67f",
   "metadata": {},
   "source": [
    "With the baseline scores for each untuned classifier, ExtraTreesClassifier had the highest accuracy so we will continue to tune with that model. First, let's look at the accuracy and metrics for an untuned ExtraTreesClassifier when k=6."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ceef78b-3d45-4304-b7a8-d8db46845fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chimp_model = Pipeline(steps=[\n",
    "    ('kmer_transformer', KMerTransformer(k=6, verbose=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier())\n",
    "])\n",
    "\n",
    "human_chimp_model.fit(X_train, y_train)\n",
    "y_preds = human_chimp_model.predict(X_test)\n",
    "\n",
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319498b-c41a-455a-9737-3145bf215417",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(font_scale = 1)\n",
    "plot_confusion_matrix(human_chimp_model, X_test, y_test)\n",
    "plt.suptitle('Human DNA vs Chimpanzee DNA ExtraTrees Confusion Matrix')\n",
    "plt.savefig('img/graphs/untuned_extratrees_confusion.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2419ef14-b3c5-4521-a436-b2b117d09d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(human_chimp_model, X_test, y_test)\n",
    "plt.suptitle('Human DNA vs Chimpanzee DNA ROC Curve')\n",
    "plt.savefig('img/graphs/untuned_extratrees_roc.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed39694-f532-4bb9-bc02-957784d2bf91",
   "metadata": {},
   "source": [
    "## K-Group K-mer Transformer\n",
    "\n",
    "Here I will create another custom transformer for preprocessing the data. Similar to the KMerTransformer, this transformer will create frequency dictionaries for each row of data. Unlike the KMerTransformer, it will pool all the A's, C's, G's, and T's counts' instead of their unique sequences.\n",
    "\n",
    "In other words, if we have a sequence like 'AAGTCGAGT' we will have 3 A's, 1 C, 3 G's, and 2 T's. So the sequence will populate a column named: 'A2C1G3T2'. Similiarly, a sequence like 'GTGAAACTG' would also populate the same column. This means that we will create many times less columns for the same k. Thus, we will increase k significantly when we call the transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7084488-5fed-40f2-983a-79583425ea05",
   "metadata": {},
   "source": [
    "With the imported KGroupKMerTransformer class, let's try it out on the data with a k=12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0492323-49f9-4b54-a852-153548cc44a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chimp_df = pd.concat([human_df, chimp_df], axis=0).reset_index(drop=True)\n",
    "\n",
    "labeler = LabelEncoder()\n",
    "\n",
    "X = human_chimp_df.drop(columns='target')\n",
    "y = labeler.fit_transform(human_chimp_df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "human_chimp_model_k12group = Pipeline(steps=[\n",
    "    ('kgroup_transformer', KGroupKMerTransformer(k=12, verbose=False)),\n",
    "    ('kgroup_column_pruner', KGroupColumnPruner()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "], verbose=True)\n",
    "\n",
    "human_chimp_model_k12group.fit(X_train, y_train)\n",
    "y_preds = human_chimp_model_k12group.predict(X_test)\n",
    "\n",
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e147f7-d074-4266-90a8-fdb3340497c5",
   "metadata": {},
   "source": [
    "As you can see, we get 86% accuracy for a fraction of the columns we used in the KMerTransformer when k=6. Let's continue this experiment and increase k to 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb28d004-24ee-4b9f-ab80-57ff9a718ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "human_chimp_model_k36group = Pipeline(steps=[\n",
    "    ('kgroup_transformer', KGroupKMerTransformer(k=36, verbose=False)),\n",
    "    ('kgroup_column_pruner', KGroupColumnPruner()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "], verbose=True)\n",
    "\n",
    "human_chimp_model_k36group.fit(X_train, y_train)\n",
    "y_preds = human_chimp_model_k36group.predict(X_test)\n",
    "\n",
    "print(classification_report(labeler.inverse_transform(y_test), labeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e6d421-e275-49d0-8644-cac196a18d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5), ncols=2)\n",
    "sns.set(font_scale = 1)\n",
    "plot_confusion_matrix(human_chimp_model_k12group, X_test, y_test, ax=ax[0])\n",
    "plot_confusion_matrix(human_chimp_model_k36group, X_test, y_test, ax=ax[1])\n",
    "ax[0].set_title('KGroups Confusion Matrix (k=12)')\n",
    "ax[1].set_title('KGroups Confusion Matrix (k=36)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864fbee5-ca1c-46f0-8ba2-f9bd5d552c20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plot_roc_curve(human_chimp_model_k12group, X_test, y_test)\n",
    "plt.suptitle('KGroups ROC-AUC (k=12)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6852d6d-f0e8-4d04-9a01-f7f9cc8e97d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(human_chimp_model_k36group, X_test, y_test)\n",
    "plt.suptitle('KGroups ROC-AUC (k=36)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd7131-96d9-498e-96e7-e7c53f61a07b",
   "metadata": {},
   "source": [
    "Now we get 90% accuracy when comparing Human and Chimpanzee DNA and k=36. After pruning columns that were never populated, we used 7386 total columns. Our AUC score is now 96% and is looking even better than in our previous model.\n",
    "\n",
    "So far, so good - let's try this model on Multi-Class data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "964c4aa9-e5c4-4816-8a1d-9d9a697f09e4",
   "metadata": {},
   "source": [
    "## Multi-Class Classification\n",
    "\n",
    "Now we will try the KGroupKMerTransformer on Multiclass data with k=36."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6292b11-f603-4f3b-921c-630f3a7ca486",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df = pd.concat([human_df, chimp_df, dolphin_df, oak_df, mushroom_df], axis=0)\n",
    "\n",
    "multilabeler = LabelEncoder()\n",
    "\n",
    "X = species_compare_df.drop(columns='target')\n",
    "y = multilabeler.fit_transform(species_compare_df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "multiclass_kgroup_model = Pipeline(steps=[\n",
    "    ('kgroup_transformer', KGroupKMerTransformer(k=36, verbose=False)),\n",
    "    ('kgroup_column_pruner', KGroupColumnPruner()),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "], verbose=True)\n",
    "\n",
    "multiclass_kgroup_model.fit(X_train, y_train)\n",
    "y_preds = multiclass_kgroup_model.predict(X_test)\n",
    "\n",
    "print('Multiclass Performance with KGroup Transformer (k=36)')\n",
    "print(classification_report(multilabeler.inverse_transform(y_test), multilabeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbdcf14-8530-45f3-b5fa-6b4803de4ba1",
   "metadata": {},
   "source": [
    "It seems that the KGroup Multiclass model is a little less accurate than the binary model. Especially in certain classes. For example, the f1-score for Mushroom is particularily low compared to the others. Let's run the original KMerTransformer model on Multiclass data and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82bced1-7a81-4637-a9e4-c0131c0e9452",
   "metadata": {},
   "outputs": [],
   "source": [
    "multiclass_kmer_model = Pipeline(steps=[\n",
    "    ('kmer_transformer', KMerTransformer(k=6, verbose=False)),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "], verbose=True)\n",
    "\n",
    "multiclass_kmer_model.fit(X_train, y_train)\n",
    "y_preds = multiclass_kmer_model.predict(X_test)\n",
    "\n",
    "print('Multiclass Performance with KMer Transformer (k=6)')\n",
    "print(classification_report(multilabeler.inverse_transform(y_test), multilabeler.inverse_transform(y_preds)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a0b60d-5bdc-42f6-8178-51f75f87804a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,5), ncols=2)\n",
    "plot_confusion_matrix(multiclass_kmer_model, X_test, y_test, ax=ax[1])\n",
    "plot_confusion_matrix(multiclass_kgroup_model, X_test, y_test, ax=ax[0])\n",
    "ax[0].set_yticklabels(labels=multilabeler.classes_)\n",
    "ax[1].set_yticklabels(labels=multilabeler.classes_)\n",
    "ax[0].set_xticklabels(labels=multilabeler.classes_, rotation=45)\n",
    "ax[1].set_xticklabels(labels=multilabeler.classes_, rotation=45)\n",
    "ax[0].set_title('K-Groups Transformer (k=36) - Accuracy 87%')\n",
    "ax[1].set_title('K-Mer Transformer (k=6) - Accuracy 89%')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3a73bd-be92-4828-8461-d3b9e68a6bfc",
   "metadata": {},
   "source": [
    "We can see in this comparison of confusion matrices that the KMerTransformer preprocessed model is much better at classifying some parts of the data that the KGroup Model has a harder time with. Let's try to combine both models into one, and compare those results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d80e8-f9fc-4964-81d8-0f28f263eb1d",
   "metadata": {},
   "source": [
    "## KGroup + KMer Model\n",
    "\n",
    "In this section, we will combine the two previous preprocessing transformers we created into one model. First, we will determine the best k for KGroups if KMer k=6.\n",
    "\n",
    "After that, we can find the best hyperparameters for the ExtraTreesClassifier using the k we find in the first grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986684b3-d0fa-4109-af6f-3e27cbab33f0",
   "metadata": {},
   "source": [
    "**Finding Best K for KGroup**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bb9770-3846-4477-96d0-00410aa01b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df = pd.concat([human_df, chimp_df, dolphin_df, oak_df, mushroom_df], axis=0)\n",
    "\n",
    "species_compare_df_sample = species_compare_df.sample(n=10000, random_state=42)\n",
    "\n",
    "multilabeler = LabelEncoder()\n",
    "\n",
    "X = species_compare_df_sample.drop(columns='target')\n",
    "y = multilabeler.fit_transform(species_compare_df_sample['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "multiclass_model = Pipeline(steps=[\n",
    "    ('trans_union', FeatureUnion([\n",
    "        ('p1', Pipeline([\n",
    "            ('kgroup_transformer', KGroupKMerTransformer(k=12, verbose=False)),\n",
    "            ('kgroup_column_pruner', KGroupColumnPruner(verbose=False)),\n",
    "        ])),\n",
    "        ('p2', Pipeline([\n",
    "            ('kmer_transformer', KMerTransformer(k=6,verbose=False))\n",
    "        ])),\n",
    "    ])),    \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "params = {\n",
    "    'trans_union__p1__kgroup_transformer__k':[6,12,24,36],\n",
    "}\n",
    "\n",
    "grid_searcher = GridSearchCV(multiclass_model, param_grid=params, cv=3, scoring='accuracy', verbose=2)\n",
    "\n",
    "grid_searcher.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04dcd0e-957b-4dfd-9e3e-9653100ffca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_est = grid_searcher.best_estimator_\n",
    "y_preds = best_est.predict(X_test)\n",
    "    \n",
    "print('Best Estimator:')\n",
    "print(best_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5895c476-4dc0-48ae-a5ca-ef81577da807",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331645ec-2d6b-4419-9bfe-7df5e573b95a",
   "metadata": {},
   "source": [
    "Based on these results. The best k for KGroup Transformer is k=24, when the KMerTransformer k=6. Let's try that in next section, and fine tune the ExtraTreesClassifier using a grid search."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e3661b-d7e4-4dc9-a03b-ca3ef5f36e91",
   "metadata": {},
   "source": [
    "**Finding best hyperparameters for ExtraTreesClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106b1fbe-baa4-4823-b08e-fb1351de552e",
   "metadata": {},
   "outputs": [],
   "source": [
    "species_compare_df = pd.concat([human_df, chimp_df, dolphin_df, oak_df, mushroom_df], axis=0)\n",
    "\n",
    "species_compare_df_sample = species_compare_df.sample(n=10000, random_state=42)\n",
    "\n",
    "multilabeler = LabelEncoder()\n",
    "\n",
    "X = species_compare_df_sample.drop(columns='target')\n",
    "y = multilabeler.fit_transform(species_compare_df_sample['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "k_trans = Pipeline([\n",
    "    ('trans_union', FeatureUnion([\n",
    "            ('p1', Pipeline([\n",
    "                ('kgroup_transformer', KGroupKMerTransformer(k=24, verbose=False)),\n",
    "                ('kgroup_column_pruner', KGroupColumnPruner(verbose=False)),\n",
    "            ])),\n",
    "            ('p2', Pipeline([\n",
    "                ('kmer_transformer', KMerTransformer(k=6, verbose=False))\n",
    "            ]))\n",
    "        ]))\n",
    "])\n",
    "\n",
    "X_train_trans = k_trans.fit_transform(X_train)\n",
    "X_test_trans = k_trans.transform(X_test)\n",
    "\n",
    "multiclass_model = Pipeline(steps=[  \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "\n",
    "params = {\n",
    "    'classifier__n_estimators':range(100,350,50),\n",
    "    'classifier__criterion':['gini', 'entropy', 'log_loss'],\n",
    "    'classifier__max_features':['sqrt','log2'],\n",
    "    'classifier__min_samples_leaf':np.linspace(1,50,8, dtype=int)\n",
    "}\n",
    "\n",
    "grid_searcher = GridSearchCV(multiclass_model, param_grid=params, cv=3, scoring='accuracy', verbose=1)\n",
    "\n",
    "with parallel_backend('threading', n_jobs=-1):\n",
    "    grid_searcher.fit(X_train_trans, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5aa422-5088-45eb-93cb-86224bef31ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_searcher.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24035752-372b-4d1d-8da4-5973cdc697b1",
   "metadata": {},
   "source": [
    "## Final Model\n",
    "\n",
    "For the final model, I will add the hyperparameters I found from the grid searches.\n",
    "\n",
    "- KGroupTransformer: k=24\n",
    "- KMerTransformer: k=6\n",
    "- ExtraTreesClassifier: n_estimators=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ed663e-3ebd-4da6-bbcc-0f9977052c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = species_compare_df.drop(columns='target')\n",
    "y = multilabeler.fit_transform(species_compare_df['target'])\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "final_model = Pipeline(steps=[\n",
    "    ('trans_union', FeatureUnion([\n",
    "        ('p1', Pipeline([\n",
    "            ('kgroup_transformer', KGroupKMerTransformer(k=24, verbose=False)),\n",
    "            ('kgroup_column_pruner', KGroupColumnPruner(verbose=False)),\n",
    "        ])),\n",
    "        ('p2', Pipeline([\n",
    "            ('kmer_transformer', KMerTransformer(k=6, verbose=False))\n",
    "        ])),\n",
    "    ])),    \n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', ExtraTreesClassifier(n_estimators=300, random_state=42))\n",
    "])\n",
    "\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "y_preds = final_model.predict(X_test)\n",
    "\n",
    "final_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df53466b-08fa-43ce-b0c2-be64fe2b1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(multilabeler.inverse_transform(y_test), multilabeler.inverse_transform(y_preds)))\n",
    "\n",
    "from yellowbrick.classifier import ClassificationReport\n",
    "\n",
    "visualizer = ClassificationReport(final_model, classes=multilabeler.classes_, support=True)\n",
    "\n",
    "visualizer.fit(X_train, y_train)        # Fit the visualizer and the model\n",
    "visualizer.score(X_test, y_test)        # Evaluate the model on the test data\n",
    "visualizer.show(outpath=\"img/graphs/final_model_classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a571ccf-c004-435a-a95b-78e8a0125510",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ClassPredictionError\n",
    "\n",
    "visualizer = ClassPredictionError(final_model, classes=multilabeler.classes_)\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show(outpath=\"img/graphs/final_model_clf_error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78645825-156c-4a52-a639-9c448958be6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix(final_model, X_test, y_test)\n",
    "plt.yticks(ticks=range(len(multilabeler.classes_)), labels=multilabeler.classes_)\n",
    "plt.xticks(ticks=range(len(multilabeler.classes_)), labels=multilabeler.classes_, rotation=45)\n",
    "plt.suptitle('Multiclass ExtraTreesClassifier - KMer + KGroup Model ')\n",
    "plt.savefig('img/graphs/final_model_confusion.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d375a4f-e1ae-40a0-aec2-cfd6c8f7a79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from yellowbrick.classifier import ROCAUC\n",
    "\n",
    "visualizer = ROCAUC(final_model, classes=multilabeler.classes_)\n",
    "\n",
    "visualizer.fit(X_train, y_train)\n",
    "visualizer.score(X_test, y_test)\n",
    "visualizer.show(outpath=\"img/graphs/final_model_roc_auc\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e301e3-4222-4a8f-8098-d94f62498ec5",
   "metadata": {},
   "source": [
    "## Results\n",
    "My final model achieved a 90% acccuracy on test data for classifying sequences between 5 genomes of DNA. Although there is room for improvement in the model, as well as trying out other classification methods (i.e. neural networks), I can reject my null hypothesis, and we can assume there are patterns of similiarity between the classes of the genomic data. \n",
    "\n",
    "Moreover, we are able to distinguish the patterns and variance and classify species from individual DNA sequences with high accuracy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
